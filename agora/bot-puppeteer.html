<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Agora Bot - Puppeteer</title>
    <script src="https://cdn.jsdelivr.net/npm/agora-rtc-sdk-ng@4.24.2/AgoraRTC_N-production.js"></script>
</head>
<body>
    <h1>Bot Running in Headless Browser</h1>
    <div id="status">Initializing...</div>

    <script>
        (async function() {
            // Wait for config to be injected
            while (!window.BOT_CONFIG) {
                await new Promise(resolve => setTimeout(resolve, 100));
            }

            const CONFIG = window.BOT_CONFIG;
            console.log('Bot config loaded:', { ...CONFIG, openAiApiKey: '***' });

            class AgoraGermanTutorBot {
                constructor() {
                    this.client = AgoraRTC.createClient({ mode: 'live', codec: 'vp8' });
                    this.remoteRecorders = new Map();
                    this.bufferSourceTrack = null;
                }

                async start() {
                    await this.client.join(CONFIG.appId, CONFIG.channel, CONFIG.token, null);
                    await this.client.setClientRole('host');
                    this._wireClientEvents();
                    console.log(`‚úÖ Bot joined channel: ${CONFIG.channel}`);
                    document.getElementById('status').textContent = `Running in channel: ${CONFIG.channel}`;
                }

                async stop() {
                    this.remoteRecorders.forEach(({ recorder, interval }) => {
                        if (recorder) recorder.stop();
                        if (interval) clearInterval(interval);
                    });
                    if (this.bufferSourceTrack) {
                        this.bufferSourceTrack.stop();
                        this.bufferSourceTrack.close();
                    }
                    await this.client.leave();
                    console.log('Bot stopped');
                }

                _wireClientEvents() {
                    this.client.on('user-published', async (user, mediaType) => {
                        await this.client.subscribe(user, mediaType);
                        if (mediaType === 'audio') {
                            const track = user.audioTrack;
                            track.play();
                            this._startRecordingTrack(user.uid, track);
                            console.log(`üé§ User ${user.uid} started speaking`);
                        }
                    });

                    this.client.on('user-unpublished', (user) => {
                        const recorderBundle = this.remoteRecorders.get(user.uid);
                        if (recorderBundle) {
                            if (recorderBundle.recorder) recorderBundle.recorder.stop();
                            if (recorderBundle.interval) clearInterval(recorderBundle.interval);
                        }
                        this.remoteRecorders.delete(user.uid);
                        console.log(`üëã User ${user.uid} stopped`);
                    });
                }

                _startRecordingTrack(uid, remoteAudioTrack) {
                    const mediaStreamTrack = remoteAudioTrack.getMediaStreamTrack();
                    const recorder = new MediaRecorder(new MediaStream([mediaStreamTrack]));
                    const chunks = [];

                    const flushAudio = async () => {
                        if (!chunks.length) return;
                        const blob = new Blob(chunks.splice(0, chunks.length), { type: 'audio/webm' });
                        await this._handleCapturedAudio(uid, blob);
                    };

                    recorder.ondataavailable = (evt) => {
                        if (evt.data.size > 0) {
                            chunks.push(evt.data);
                        }
                    };

                    recorder.onstop = () => {
                        flushAudio();
                    };

                    recorder.start();
                    const interval = setInterval(flushAudio, 8000);

                    this.remoteRecorders.set(uid, { recorder, interval });
                }

                async _handleCapturedAudio(uid, blob) {
                    try {
                        const transcription = await this._transcribe(blob);
                        if (!transcription?.trim()) return;

                        console.log(`üìù Transcribed from ${uid}: "${transcription}"`);
                        const reply = await this._generateReply(transcription);
                        console.log(`üó£Ô∏è Bot reply: "${reply}"`);
                        await this._speak(reply);
                    } catch (err) {
                        console.error('‚ùå Error processing audio:', err.message);
                    }
                }

                async _transcribe(blob) {
                    const formData = new FormData();
                    formData.append('file', blob, 'segment.webm');
                    formData.append('model', 'whisper-1');
                    formData.append('language', 'de');
                    formData.append('prompt', 'German language learning conversation');

                    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${CONFIG.openAiApiKey}`,
                        },
                        body: formData
                    });

                    if (!response.ok) {
                        throw new Error(`Transcription failed: ${response.statusText}`);
                    }

                    const data = await response.json();
                    return data?.text || '';
                }

                async _generateReply(text) {
                    const response = await fetch('https://api.openai.com/v1/chat/completions', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': `Bearer ${CONFIG.openAiApiKey}`,
                        },
                        body: JSON.stringify({
                            model: 'gpt-4-turbo-preview',
                            messages: [
                                {
                                    role: 'system',
                                    content: 'Du bist eine freundliche, geduldige deutsche Lehrerin. ' +
                                        'Korrigiere behutsam, erkl√§re Grammatik knapp, biete neue Vokabeln ' +
                                        'und stelle R√ºckfragen, um die Unterhaltung lebendig zu halten.',
                                },
                                { role: 'user', content: text },
                            ],
                        })
                    });

                    if (!response.ok) {
                        throw new Error(`Chat completion failed: ${response.statusText}`);
                    }

                    const data = await response.json();
                    return data.choices[0].message.content.trim();
                }

                async _speak(text) {
                    const response = await fetch('https://api.openai.com/v1/audio/speech', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': `Bearer ${CONFIG.openAiApiKey}`,
                        },
                        body: JSON.stringify({
                            model: 'tts-1',
                            voice: 'alloy',
                            input: text,
                            response_format: 'wav',
                        })
                    });

                    if (!response.ok) {
                        throw new Error(`TTS failed: ${response.statusText}`);
                    }

                    const audioBuffer = await response.arrayBuffer();
                    const ctx = new AudioContext();
                    const decoded = await ctx.decodeAudioData(audioBuffer);
                    const source = ctx.createBufferSource();
                    source.buffer = decoded;
                    
                    const track = AgoraRTC.createBufferSourceAudioTrack({ source });
                    await this.client.publish(track);
                    source.start();
                    this.bufferSourceTrack = track;
                }
            }

            // Start the bot
            try {
                const bot = new AgoraGermanTutorBot();
                await bot.start();
                window.botInstance = bot;
                window.botReady = true;
                console.log('‚úÖ Bot is ready and running');
            } catch (error) {
                console.error('‚ùå Failed to start bot:', error);
                window.botReady = false;
                throw error;
            }
        })();
    </script>
</body>
</html>
